{
    "name": "Source 1.0", 
    "series": "Source", 
    "organization": "Inspur", 
    "modality": "NLP", 
    "publication": {
        "date": "2021-10-11", 
        "link": "https://www.gwern.net/docs/ai/scaling/2021-10-11-xinzhiyuan-inspursource10gpt245b.html", 
        "name": "The world's largest AI massive model 'Source 1.0' is released, and Inspur is bravely climbing AI Everest"
    }, 
    "model": {
        "link": "", 
        "type": "decoder", 
        "details": "", 
        "parameters": 245.7, 
        "availability": "not available"
    }, 
    "training": {
        "compute": 4095, 
        "hardware": "?", 
        "framework": "?", 
        "provider": "Inspur", 
        "code_availability": "unavailable", 
        "code_link": "https://github.com/huggingface/transformers", "https://github.com/NVIDIA/Megatron-LM, https://github.com/pytorch/fairseq"
    }, 
    "dataset": {
        "unique_tokens": 3000, 
        "name": "custom", 
        "content": "Chinese", 
        "link": "",
        "training_tokens": 240, 
        "availability": "unavailable", 
        "size": 5000
    }
}
