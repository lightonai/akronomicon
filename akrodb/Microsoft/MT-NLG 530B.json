{
    "name": "MT-NLG 530B", 
    "series": "Megatron-Turing", 
    "organization": "Microsoft, NVIDIA", 
    "modality": "NLP", 
    "publication": {
        "date": "2021-10-11", 
        "link": "https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/", 
        "name": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the Worldâ€™s Largest and Most Powerful Generative Language Model"
    }, 
    "model": {
        "link": "", 
        "type": "decoder", 
        "details": "", 
        "parameters": 503, 
        "availability": "not available"
    }, 
    "training": {
        "compute": "120", 
        "hardware": "NVIDIA A100", 
        "framework": "PyTorch, DeepSpeed, Megatron", 
        "provider": "NVIDIA, Selene", 
        "code_availability": "not available", 
        "code_link": ""
    }, 
    "dataset": {
        "unique_tokens": "339", 
        "name": "The Pile (extended)", 
        "content": "English", 
        "link": "", 
        "training_tokens": "270", 
        "availability": "reproducible", 
        "size": "?"
    }
}
