{
    "name": "Megatron-Turing NLG 530B", 
    "series": "", 
    "organization": "Microsoft, NVIDIA", 
    "modality": "NLP", 
    "publication": {
        "date": "2021-10-11", 
        "link": "https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/", 
        "name": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the Worldâ€™s Largest and Most Powerful Generative Language Model"
    }, 
    "model": {
        "link": "", 
        "type": "", 
        "details": "", 
        "parameters": 503, 
        "availability": "not available"
    }, 
    "training": {
        "compute": "120", 
        "hardware": "NVIDIA A100 Tensor Core GPUs and HDR InfiniBand networking", 
        "framework": "DeepSpeed, Megatron", 
        "provider": "N/A", 
        "code_availability": "unavailable", 
        "code_link": ""
    }, 
    "dataset": {
        "unique_tokens": "N/A", 
        "name": "various (Wikipedia, Books3, ArXis and so on)", 
        "content": "English", 
        "link": "N/A", 
        "training_tokens": "N/A", 
        "availability": "public", 
        "size": "N/A"
    }
}
