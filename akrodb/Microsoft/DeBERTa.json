{
    "name": "DeBERTa", 
    "series": "N/A", 
    "organization": "Microsoft", 
    "modality": "NLP", 
    "publication": {
        "date": "2021-10-04", 
        "link": "https://arxiv.org/abs/2006.03654", 
        "name": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
    }, 
    "model": {
        "link": "", 
        "type": "encoder", 
        "details": "", 
        "parameters": 1.5, 
        "availability": "not available"
    }, 
    "training": {
        "compute": "128", 
        "hardware": "V100", 
        "framework": "Fairseq, Huggingface Transformers, Megatron", 
        "provider": "Microsoft", 
        "code_availability": "not available", 
        "code_link": "",
    }, 
    "dataset": {
        "unique_tokens": "?", 
        "name": "custom", 
        "content": "English", 
        "link": "",
        "training_tokens": "?", 
        "availability": "not available", 
        "size": "160G"
    }
}
