{
    "name": "M6-10T", 
    "series": "M6", 
    "organization": "", 
    "modality": "NLP", 
    "publication": {
        "date": "2021-09-29", 
        "link": "https://openreview.net/forum?id=TXqemS7XEH", 
        "name": "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining"
    }, 
    "model": {
        "link": "", 
        "type": "encoder", 
        "details": "", 
        "parameters": 10000, 
        "availability": "not available"
    }, 
    "training": {
        "compute": "160", 
        "hardware": "V100 32GB", 
        "framework": "Fairseq, Huggingface Transformers, Megatron", 
        "provider": "N/A",
        "code_availability": "available", 
        "code_link": "https://github.com/huggingface/transformers", "https://github.com/NVIDIA/Megatron-LM, https://github.com/pytorch/fairseq"
    }, 
    "dataset": {
        "unique_tokens": "N/A", 
        "name": "Wikipedia, BookCorpus", 
        "content": "English", 
        "link": "",
        "training_tokens": "N/A", 
        "availability": "public", 
        "size": "16"
    }
}
